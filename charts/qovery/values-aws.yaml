services:
  qovery:
    qovery-cluster-agent:
      enabled: true
    qovery-shell-agent:
      enabled: true
    qovery-engine:
      enabled: true
    qovery-priority-class:
      enabled: true
  ingress:
    ingress-nginx:
      enabled: true
  dns:
    external-dns:
      enabled: true
  logging:
    loki:
      enabled: true
    promtail:
      enabled: true
  certificates:
    cert-manager:
      enabled: true
    cert-manager-configs:
      enabled: true
    qovery-cert-manager-webhook:
      enabled: true
  observability:
    metrics-server:
      enabled: true
  aws:
    q-storageclass-aws:
      enabled: true
    aws-ebs-csi-driver:
      enabled: false
    aws-load-balancer-controller:
      enabled: false
  gcp:
    q-storageclass-gcp:
      enabled: false
  scaleway:
    q-storageclass-scaleway:
      enabled: false
qovery:
  clusterId: &clusterId set-by-customer
  clusterShortId: &clusterShortId set-by-customer
  organizationId: &organizationId set-by-customer
  jwtToken: &jwtToken set-by-customer
  rootDomain: &rootDomain set-by-customer
  domain: &domain set-by-customer
  domainWildcard: &domainWildcard set-by-customer
  qoveryDnsUrl: &qoveryDnsUrl set-by-customer
  agentGatewayUrl: &agentGatewayUrl set-by-customer
  engineGatewayUrl: &engineGatewayUrl set-by-customer
  lokiUrl: &lokiUrl set-by-customer
  promtailLokiUrl: &promtailLokiUrl set-by-customer
  acmeEmailAddr: &acmeEmailAddr set-by-customer
  externalDnsPrefix: &externalDnsPrefix set-by-customer
  architectures: &architectures set-by-customer
  engineVersion: &engineVersion set-by-customer
  shellAgentVersion: &shellAgentVersion set-by-customer
  clusterAgentVersion: &clusterAgentVersion set-by-customer
qovery-cluster-agent:
  fullnameOverride: qovery-shell-agent
  image:
    tag: *clusterAgentVersion
  environmentVariables:
    CLUSTER_ID: *clusterId
    CLUSTER_JWT_TOKEN: *jwtToken
    GRPC_SERVER: *agentGatewayUrl
    LOKI_URL: *lokiUrl
    ORGANIZATION_ID: *organizationId
  useSelfSignCertificate: true
qovery-shell-agent:
  fullnameOverride: qovery-shell-agent
  image:
    tag: *shellAgentVersion
  environmentVariables:
    CLUSTER_ID: *clusterId
    CLUSTER_JWT_TOKEN: *jwtToken
    GRPC_SERVER: *agentGatewayUrl
    ORGANIZATION_ID: *organizationId
qovery-engine:
  image:
    tag: *engineVersion
  engineResources: null
  buildContainer:
    environmentVariables:
      BUILDER_CPU_ARCHITECTURES: *architectures
      BUILDER_ROOTLESS_ENABLED: 'true'
  environmentVariables:
    CLUSTER_ID: *clusterId
    CLUSTER_JWT_TOKEN: *jwtToken
    DOCKER_HOST: tcp://0.0.0.0:2375
    GRPC_SERVER: *engineGatewayUrl
    LIB_ROOT_DIR: /home/qovery/lib
    ORGANIZATION_ID: *organizationId
ingress-nginx:
  controller:
    useComponentLabel: true
    admissionWebhooks:
      enabled: true # set-by-customer
    allowSnippetAnnotations: true
    # enable if you want metrics scrapped by prometheus
    metrics:
      enabled: true # set-by-customer
      serviceMonitor:
        enabled: false # set-by-customer
    config:
      # set global default file size limit to 100m
      proxy-body-size: 100m
      # hide Nginx version
      server-tokens: "false"
      # annotation risk level - needed because of nginx.ingress.kubernetes.io/configuration-snippet in ingress configuration
      annotations-risk-level: "Critical"
      # required for X-Forwarded-for to work with ALB controller
      use-proxy-protocol: "false"
      # enable real IP (client IP)
      enable-real-ip: "false" # set-by-customer
      # passes the incoming X-Forwarded-* headers to upstreams
      use-forwarded-headers: "false" # set-by-customer
      # append the remote address to the X-Forwarded-For header instead of replacing it
      compute-full-forwarded-for: "false" # set-by-customer
      # customize http-snippet
      # customize log-format / set-by-customer
      # default format can be found in the template: https://github.com/nginxinc/kubernetes-ingress/blob/v3.5.2/internal/configs/version1/nginx.tmpl#L44
      # nginx_controller_log_format_upstream can be a json that why we pass it in the value file
      log-format-escaping-json: "false" # set-by-customer
      log-format-escaping-none: "false" # set-by-customer
      limit-req-status-code: "503" # set-by-customer
      # Compression configuration (enabled by default)
      enable-brotli: "true"
      brotli-level: "6"
      brotli-types: "text/xml text/yaml image/svg+xml application/x-font-ttf image/vnd.microsoft.icon application/x-font-opentype application/json font/eot application/vnd.ms-fontobject application/javascript font/otf application/xml application/xhtml+xml text/javascript application/x-javascript text/plain application/x-font-truetype application/xml+rss image/x-icon font/opentype text/css image/x-win-bitmap"
      use-gzip: "true"
      gzip-level: "6"
      gzip-types: "text/xml text/yaml image/svg+xml application/x-font-ttf image/vnd.microsoft.icon application/x-font-opentype application/json font/eot application/vnd.ms-fontobject application/javascript font/otf application/xml application/xhtml+xml text/javascript application/x-javascript text/plain application/x-font-truetype application/xml+rss image/x-icon font/opentype text/css image/x-win-bitmap"
    # PDB
    maxUnavailable: 20%
    ingressClassResource:
      # -- Name of the IngressClass
      name: nginx-qovery
      # -- Create the IngressClass or not
      enabled: true
    # the Ingress Class name to be used by Ingresses (use "nginx-qovery" for Qovery application/container deployments)
    ingressClass: nginx-qovery
    extraArgs:
      # Kubernetes path of the default Cert-manager TLS certificate (if used)
      default-ssl-certificate: "qovery/letsencrypt-acme-qovery-cert"
    updateStrategy:
      rollingUpdate:
        # AWS LB is slow to catchup change in the topology, so we go 1 by 1 to not have any downtime
        maxSurge: 1
        maxUnavailable: 0
    # AWS LB is slow to catchup change in the topology, so we go slowly to let AWS catchup change before moving to the next instance
    # LB healthcheck is 6, and need 2 rounds to consider the instance as (un)healthy. Double the time to be safe
    readinessProbe:
      initialDelaySeconds: 30
    # enable autoscaling if you want to scale the number of replicas based on CPU usage
    autoscaling:
      enabled: false # set-by-customer
      minReplicas: 2 # set-by-customer
      maxReplicas: 25 # set-by-customer
      targetCPUUtilizationPercentage: 50 # set-by-customer
    # required if you rely on a load balancer
    # the controller mirrors the address of this service's endpoints to the load-balancer status of all Ingress objects it satisfies.
    publishService:
      enabled: true
    # set a load balancer if you want your Nginx to be publicly accessible
    service:
      enabled: true
      annotations:
        # Qovery is in a migration process to ALB controller, we advice to use it
        service.beta.kubernetes.io/aws-load-balancer-type: nlb
        # service.beta.kubernetes.io/aws-load-balancer-type: "external"
        # service.beta.kubernetes.io/aws-load-balancer-scheme": "internet-facing"
        # service.beta.kubernetes.io/aws-load-balancer-name: "qovery-<short-cluster-id>-nginx-ingress",
        # service.beta.kubernetes.io/aws-load-balancer-proxy-protocol": "*",
        # service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "ip",
        # service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true",
        # service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: "target_health_state.unhealthy.connection_termination.enabled=false,target_health_state.unhealthy.draining_interval_seconds=300",
        # service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: "10",
        # Qovery managed DNS requieres *.$domain (something like: *.<cluster_id>.<given_dns_name>)
        external-dns.alpha.kubernetes.io/hostname: *domainWildcard
      externalTrafficPolicy: "Local"
      sessionAffinity: ""
      healthCheckNodePort: 0
    # force a connection for 30 seconds before shutting down, to avoid exiting too early
    # and let time to AWS LB to catchup change in the topology
    # When /wait-shutdown is called, the LB healthcheck /healthz endpoint return an error, but nginx keep processing request
    lifecycle:
      preStop:
        exec:
          command:
            - sh
            - -c
            - (sleep 30 | nc localhost 80)&  sleep 1 ; /wait-shutdown
    topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/instance: nginx-ingress
            app.kubernetes.io/component: controller
        topologyKey: kubernetes.io/hostname
        maxSkew: 1
        whenUnsatisfiable: DoNotSchedule
  
external-dns:
  global:
    security:
      ## @param global.security.allowInsecureImages Allows skipping image verification to use the image from our registry
      allowInsecureImages: true
  # set the provider to use
  provider: set-by-customer
  # keep the config you want to use and remove the others. Configure the provider you want to use.
  cloudflare:
    apiToken: set-by-customer
    email: set-by-customer
    proxied: set-by-customer
  pdns:
    # Qovery DNS: apiUrl: *qoveryDnsUrl
    apiUrl: set-by-customer
    # Qovery DNS: apiPort: "443"
    apiPort: set-by-customer
    # Qovery DNS: apiKey: "443"
    apiKey: set-by-customer
  # We have only 1 instance of external-dns, so creating pdb benefits nothing and may create unnecessary constraints
  # i.e: Karpenter will take them into account
  pdb:
    create: false
  # Make external DNS ignore this ingress https://github.com/kubernetes-sigs/external-dns/issues/1910#issuecomment-976371247
  annotationFilter: external-dns.alpha.kubernetes.io/exclude notin (true)
  # set domainFilters to the domain you want to manage: [*domain]
  domainFilters: set-by-customer
  triggerLoopOnEvent: true
  policy: sync
  # avoid dns collision with other external-dns instances
  txtOwnerId: set-by-customer
  txtPrefix: set-by-customer
  # set the number of replicas you want to use
  replicas: 1
  # set the rolling update strategy you want to apply
  updateStrategy:
    type: set-by-customer
  # remove if you don't want to use a custom image
  image:
    registry: set-by-customer
    repository: set-by-customer
  # set resources
  resources:
    limits:
      cpu: 50m
      memory: 100Mi
    requests:
      cpu: 50m
      memory: 100Mi
  
promtail:
  # remove if you don't want to use a custom image
  image:
    registry: set-by-customer
    repository: set-by-customer
  # It's mandatory to get this class to ensure paused infra will behave properly on restore
  priorityClassName: system-node-critical
  config:
    clients:
      # set loki URL: *promtailLokiUrl or set a custom URL
      - url: set-by-customer
    snippets:
      extraRelabelConfigs:
        # required this config in order for the cluster agent to retrieve the log of the service
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(qovery_com_service_id|qovery_com_service_type|qovery_com_environment_id)
  # set resources
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 128Mi
  
loki:
  # remove if you don't want to use a custom image
  kubectlImage:
    registry: set-by-customer
    repository: set-by-customer
  tableManager:
    retention_deletes_enabled: set-by-customer # qovery setting
    retention_period: set-by-customer # qovery setting
  loki:
    podLabels: { }
    # remove if you don't want to use a custom image
    image:
      registry: set-by-customer
      repository: set-by-customer
    # set if you want to use authentication
    auth_enabled: false
    commonConfig:
      # for simple usage, without high throughput, you can use the 1 replica only
      # note: replication is assured by the storage backend
      replication_factor: 1
    ingester:
      chunk_idle_period: 3m
      chunk_block_size: 262144
      chunk_retain_period: 1m
      max_transfer_retries: 0
      lifecycler:
        ring:
          kvstore:
            store: memberlist
          replication_factor: 1
    memberlistConfig:
      abort_if_cluster_join_fails: false
      bind_port: 7946
      join_members:
        # set loki headless service
        - loki-headless.logging.svc:7946
      max_join_backoff: 1m
      max_join_retries: 10
      min_join_backoff: 1s
    limits_config:
      ingestion_rate_mb: 20
      ingestion_burst_size_mb: 30
      enforce_metric_name: false
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      max_concurrent_tail_requests: 100
      split_queries_by_interval: 15m
      max_query_lookback: 12w
    compactor:
      working_directory: /data/retention
      # configure storage provider for the compactor
      shared_store: set-by-customer
      compaction_interval: 10m
      retention_enabled: set-by-customer
      retention_delete_delay: 2h
      retention_delete_worker_count: 150
    schema_config:
      configs:
        # set the schema for the index (2020 version can be deleted on a fresh install)
        - from: 2020-05-15
          store: boltdb-shipper
          object_store: s3
          schema: v11
          index:
            prefix: index_
            period: 24h
        - from: 2023-06-01
          store: boltdb-shipper
          object_store: s3
          schema: v12
          index:
            prefix: index_
            period: 24h
    storage:
      # configure the object storage backend
      bucketNames:
        chunks: set-by-customer
        ruler: set-by-customer
        admin: set-by-customer
      type: set-by-customer
      s3:
        s3: set-by-customer
        region: set-by-customer
        s3ForcePathStyle: set-by-customer
        insecure: set-by-customer
      gcs:
        chunkBufferSize: 0
        requestTimeout: "0s"
        enableHttp2: true
      azure:
        account_name: set-by-customer
        container_name: set-by-customer
        use_federated_token: true # Use federated token for authentication
    storage_config:
      gcs:
        bucket_name: set-by-customer
      boltdb_shipper:
        active_index_directory: /data/loki/index
        shared_store: set-by-customer
        resync_interval: 5s
        cache_location: /data/loki/boltdb-cache
      azure:
        account_name: set-by-customer
        container_name: set-by-customer
        use_federated_token: true # Use federated token for authentication
  monitoring:
    dashboards:
      enabled: false
    rules:
      enabled: false
    serviceMonitor:
      enabled: false
      metricsInstance:
        enabled: false
    selfMonitoring:
      enabled: false
      grafanaAgent:
        installOperator: false
    lokiCanary:
      enabled: false
  test:
    enabled: false
  gateway:
    enabled: false
  # set the single binary version for basic usage
  singleBinary:
    replicas: 1
    # set resources
    resources:
      limits:
        cpu: set-by-customer
        memory: set-by-customer
      requests:
        cpu: set-by-customer
        memory: set-by-customer
    persistence:
      enabled: false
    extraVolumes:
      - name: data
        emptyDir: { }
      - name: storage
        emptyDir: { }
    extraVolumeMounts:
      - name: data
        mountPath: /data
      - name: storage
        mountPath: /var/loki
    # set disk persistence to reduce data loss in case of pod crash
    # persistence:
    #   storageClass: set-by-customer
  serviceAccount:
    name: null
    labels: { }
    annotations: { }
  
cert-manager:
  global:
    leaderElection:
      # ensure there is only one active controller manager per cluster. Set where the leader election lock is stored.
      namespace: set-by-customer
  # Qovery requires CRD
  # WARNING: do not enable this if you already have another Cert-manager installed on the cluster to avoid CRD conflicts version.
  # Qovery strongly discourage to use another Cert-manager to reduce configuration complexity and miss-configuration.
  crds:
    # This option decides if the CRDs should be installed
    # as part of the Helm installation.
    enabled: true
    # This option makes it so that the "helm.sh/resource-policy": keep
    # annotation is added to the CRD. This will prevent Helm from uninstalling
    # the CRD when the Helm release is uninstalled.
    # WARNING: when the CRDs are removed, all cert-manager custom resources
    # (Certificates, Issuers, ...) will be removed too by the garbage collector.
    keep: true
  config:
    featureGates:
      # Disable the use of Exact PathType in Ingress resources, to work around a bug in ingress-nginx
      # https://github.com/kubernetes/ingress-nginx/issues/11176
      ACMEHTTP01IngressPathTypeExact: false
  startupapicheck:
    jobAnnotations:
      helm.sh/hook: post-install,post-upgrade
    rbac:
      annotations:
        helm.sh/hook: post-install,post-upgrade
    serviceAccount:
      annotations:
        helm.sh/hook: post-install,post-upgrade
  # enable if you want Prometheus scraping
  prometheus:
    servicemonitor:
      enabled: set-by-customer
      prometheusInstance: qovery
  # Qovery DNS are managed by Cloudflare, so to speed up DNS availability, we use Cloudflare DNS + Google DNS as fallback.
  # But you can use the one you want (it may slowdown application deployment because of DNS check).
  dns01RecursiveNameserversOnly: true
  dns01RecursiveNameservers: "1.1.1.1:53,8.8.8.8:53"
  # configure the number of instances
  replicaCount: 1
  # set rolling restart strategy
  strategy:
    type: set-by-customer
  # set resources for the controller-manager
  resources:
    limits:
      cpu: set-by-customer
      memory: set-by-customer
    requests:
      cpu: set-by-customer
      memory: set-by-customer
  # set resources for the webhook
  webhook:
    strategy:
      type: set-by-customer
    resources:
      limits:
        cpu: set-by-customer
        memory: set-by-customer
      requests:
        cpu: set-by-customer
        memory: set-by-customer
  # set resources for the cainjector
  cainjector:
    strategy:
      type: set-by-customer
    resources:
      limits:
        cpu: set-by-customer
        memory: set-by-customer
      requests:
        cpu: set-by-customer
        memory: set-by-customer
  
qovery-cert-manager-webhook:
  certManager:
    # set the same namespace than cert-manager
    namespace: set-by-customer
    serviceAccountName: cert-manager
  # set the rolling restart strategy
  updateStrategy:
    type: set-by-customer
  secret:
    # set *qoveryDnsUrl (Qovery DNS)
    apiUrl: set-by-customer
    # set *jwtToken (Qovery DNS)
    apiKey: set-by-customer
  # set resources
  resources:
    limits:
      cpu: set-by-customer
      memory: set-by-customer
    requests:
      cpu: set-by-customer
      memory: set-by-customer
  
cert-manager-configs:
  # set the same namespace than cert-manager
  namespace: set-by-customer
  # configure the provider and the managed DNS (pdns for Qovery DNS)
  externalDnsProvider: set-by-customer
  # you can use [*domain] or the ones you want
  managedDns: set-by-customer
  acme:
    letsEncrypt:
      # Let's encrypt email address for notifications
      emailReport: set-by-customer
      # set the Let's Encrypt URL
      # Test: https://acme-staging-v02.api.letsencrypt.org/directory
      # Prod: https://acme-v02.api.letsencrypt.org/directory
      acmeUrl: set-by-customer
  # configure the provider, remove the one you don't use (generally same config as external DNS)
  provider:
    cloudflare:
      apiToken: set-by-customer
      email: set-by-customer
    pdns:
      # Qovery DNS: apiPort: "443"
      apiPort: set-by-customer
      # Qovery DNS: apiUrl: *qoveryDnsUrl
      apiUrl: set-by-customer
      # Qovery DNS: apiKey: *jwtToken
      apiKey: set-by-customer
  
metrics-server:
  # create api service to be able to use hpa/vpa
  apiService:
    create: true
  # set rolling restart strategy
  updateStrategy:
    type: RollingUpdate # set-by-customer
  args: [] # set-by-customer
  # set resources
  resources:
    limits:
      cpu: 250m # set-by-customer
      memory: 64Mi # set-by-customer
    requests:
      cpu: 250m # set-by-customer
      memory: 64Mi # set-by-customer
  
